{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon 2\n",
    "\n",
    "In the first exercise, you will develop a model for estimating the cooling/heating load indexes of a building from its characteristics. These indexes serve as reference to adapt the power of air conditioning systems. In the second exercise, you will forecast the hourly electric power consumption in Spain. Electricity being not storable, predicting the electric consumption is a crucial matter for energy producers. You will develop an autoregressive model for this purpose. \n",
    "\n",
    "## Report content\n",
    "\n",
    "‚Ä¢\tYou have to fill in this  jupyter notebook downloadable on the moodle website of the course\n",
    "\n",
    "‚Ä¢\tGrades are granted to the members whose names are in the Jupyter notebook. If your name doesn‚Äôt appear on the top of the notebook, you‚Äôll get a 0, even though you are in a group on Moodle.\n",
    "\n",
    "‚Ä¢\tThe jupyter notebook must be compiled with printed results and next submitted via moodle. The absence of compiled results (or non-printed values) leads to a lower grade.\n",
    "\n",
    "## Report submission\n",
    "\n",
    "‚Ä¢\tThe deadline for submission is reported on the moodle website. Submission after the deadline will not be accepted.\n",
    "\n",
    "‚Ä¢\tTo submit your report, go to the section ‚ÄúAPP‚Äù on Moodle and the subsection ‚ÄúSoumission du rapport‚Äù. You can upload your work there. Once you are sure that it is your final version, click the button ‚ÄúEnvoyer le devoir‚Äù. It is important that you don‚Äôt forget to click on this button ! \n",
    "\n",
    "‚Ä¢\tReports that have not been uploaded through Moodle will not be corrected.\n",
    "\n",
    "## Names and Noma of participants:\n",
    "\n",
    "Part. 1: Sacha Defr√®re (51621900)\n",
    "\n",
    "Part. 2: Alexandre Pirot (53811900)\n",
    "\n",
    "Part. 3: Thomas Hautier (80162000)\n",
    "\n",
    "Part. 4: Bryce Burignat (35171700)\n",
    "\n",
    "Part. 5: Math√©o Ketels (23782000)\n",
    "\n",
    "Part. 6: Dylan Goffinet (08471900)\n",
    "\n",
    "\n",
    "## Regression\n",
    "\n",
    "When designing a building, the computation of the heating load (HL) and the cooling load (CL) is required to determine the specifications of the heating and cooling equipment needed to maintain comfortable indoor air conditions. Architects and building designers need information about the characteristics of the building and of the conditioned space. For this reason, we  investigate the effect of eight input variables: (RC), surface area, wall area, roof area, overall height, orientation, glazing area, and glazing area distribution, to determine the output variables HL and CL of residential buildings.\n",
    "The dataset contains eight attributes (or features) and two responses (or outcomes). The aim is to use the eight features to predict each of the two responses.\n",
    "\n",
    "Features: RelativeCompactness, SurfaceArea ,WallArea, RoofArea, OverallHeight, Orientation, GlazingArea, GlazingAreaDistribution.\n",
    "\n",
    "Prediction: HeatingLoad, CoolingLoad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1) Report useful statistics for each variables (mean, std, heatmap of correlations,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn\n",
    "\n",
    "df = pd.read_csv(\"Data_heating_cooling.csv\")\n",
    "print(df.describe(percentiles=[.05, .25, .5, .75, .95]))\n",
    "corrMatrix = df.corr()\n",
    "seaborn.heatmap(corrMatrix, annot=True, fmt=\".2f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation heatmap gives us some interesting information :\n",
    "- There are features which seem very independent from the others, such as the orientation, the glazing area and the galzing area distribution. Those features also have little impact on our prediction loads compared to other features.\n",
    "- Via the two rightmost columns, we can also sort our features based on their impact/correlation on our predictions :\n",
    "    - Positive effect : from highest correlation to lowest, we have the overall height, the relative compactness, the wall area and the glazing area.\n",
    "    - Negative effect : from highest correlation (absolute value) to lowest, we have the roof area and the surface area.\n",
    "    - Negligible effect : the orientation and the glazing area distribution have little to no effect on our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2)\tAdd a constant term (intercept) and perform a linear regression of the variable CoolingLoad on all other explanatory variables. Use  the library statsmodels.\n",
    "\n",
    "i. Report the F statistics and R2: interpret them. \n",
    "\n",
    "ii. Analyze the t-statistics and p-values of each coefficient of regression.\n",
    "\n",
    "Are all coefficients significant at 95%? Use the library statsmodels.api. The function OLS accepts pandas dataframe (use .drop() to remove columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df_feat = df.drop(['Cooling_Load', 'Heating_Load'], axis=1)\n",
    "df_feat = sm.add_constant(df_feat)\n",
    "X = df_feat.values.tolist()\n",
    "Y = df.filter(['Cooling_Load']).values.tolist()\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(model.summary())\n",
    "\n",
    "print(f\"\\n\\nF-statistic value : {model.fvalue}\\nAssociated p-value : {model.f_pvalue}\")\n",
    "print(f\"R¬≤ value : {model.rsquared}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F-statistic of our regression and its associated p-value are respectively 2197.2054050283373 and 0.0.\n",
    "\n",
    "The R¬≤ of our regression is 0.9586074730223842.\n",
    "\n",
    "Our F-statistic value is supposed to be interpreted with regard to the F-critical value (dependant on the degrees of freedom of our set), which has not been computed here. However, we can look at the p-value associated with the F-statistic : since it is nul (and more particularly inferior to 0.05, our significance level), we can say that one or more of our features is meaningful and significant for our model. In otherwords, the null hypothesis is rejected.\n",
    "\n",
    "We can interpret our high R¬≤ value as our model being able to explain the observed data and its variances really well, i.e. its coefficient are well-suited to the data.\n",
    "\n",
    "As for the t and p-values, the analysis is similar to the F-value : t-values are a measurement of how precise our coefficients are with regard to the t-critical value, which we do not have. However, the associated p-values tell us how likely it is that each feature indeed has an impact on our predictions. With our 95% confidence level, we can filter out the non-significant features, which only removes the orientation and the glazing area distribution, the two features that were already suspected to be unrelated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3)\tRemove non-significant coefficients except the intercept (e.g. with  ùõº  5%) and run again the regression. What do you notice when you compare the R2, log-likelihood, AIC and BIC (the AIC and BIC are not explained in the course, search on internet for explanations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_bis = df_feat.drop(['Orientation', 'Glazing_Area_Distribution'], axis=1)\n",
    "Xbis = df_feat_bis.values.tolist()\n",
    "\n",
    "model_bis = sm.OLS(Y, Xbis).fit()\n",
    "print(model_bis.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that no value (R¬≤, LL, AIC and BIC) has changed by a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-regressive model\n",
    "\n",
    "In a paper released early 2019, forecasting in energy markets is identified as one of the highest leverage contribution areas of Machine/Deep Learning toward transitioning to a renewable based electrical infrastructure (see https://arxiv.org/abs/1906.05433).  The file ‚ÄúData_energy_load.csv‚Äù contains 4 years of electrical consumption  for Spain in MW/h.  This was retrieved from ENTSOE a public portal for Transmission Service Operator (TSO) data.\n",
    "\n",
    "File format:\n",
    "\n",
    "Date: dd-mm-yy, Hour : from 0 to 23. Weekday: from 1 (Monday)  to 7 (Sunday). Load: consumption in MW/h. Lm1: consumption 1h ago, Lm2: consumption 2h ago,‚Ä¶ Lm14: consumption 14h ago\n",
    "\n",
    "1)\tLoad the dataset and convert dates in datetime format (you can use the package datetime). Plot the time series of consumption, what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"Data_energy_load.csv\", sep=\";\")\n",
    "\n",
    "# Dates in Datetime format\n",
    "def check_date(x):\n",
    "    date = x.strip().split('-')\n",
    "    return datetime.datetime(2000+int(date[2]), int(date[1]), int(date[0]))\n",
    "df[\"Date\"] = df[\"Date\"].transform(check_date)\n",
    "# Renaming LM12, LM13 and LM14 (they have upper-case \"M\" instead of lower-case \"m\")\n",
    "df.rename(columns = {'LM12':'Lm12', 'LM13':'Lm13', 'LM14':'Lm14'}, inplace = True)\n",
    "print(df.head())\n",
    "\n",
    "# Global plot of consumption\n",
    "X = np.arange(len(df))\n",
    "Y = df[\"Load\"].to_list()\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(X, Y)\n",
    "plt.xlabel(\"Hours\")\n",
    "plt.ylabel(\"Load\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the plot, it seems that the consumption varies randomly throughout the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2) We will regress the electric consumption (‚ÄòLoad‚Äô) on the following covariates ‚ÄòWeekday‚Äô, ‚ÄòHour‚Äô, ‚ÄòLm1‚Äô, ‚ÄòLm2‚Äô,‚Ä¶,‚ÄôLm14‚Äô. Plot the following graphs:\n",
    "\n",
    "‚Ä¢\tAverage electric consumption per day (y axis) versus weekday (x axis)\n",
    "\n",
    "‚Ä¢\tAverage electric consumption per hour(y axis) versus hour (x axis)\n",
    "\n",
    "Based on these graphs, do you think that it is a good idea to regress linearly the consumption on variables ‚ÄòHour‚Äô and ‚ÄòWeekday‚Äô? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice plots\n",
    "Weekdays = np.arange(1,8)\n",
    "Av_weekday = [df[df[\"Weekday\"] == i][\"Load\"].mean() for i in range(1, 8)]\n",
    "Hours = np.arange(24)\n",
    "Av_hour = [df[df[\"Hour\"] == i][\"Load\"].mean() for i in range(24)]\n",
    "\n",
    "plt.plot(Weekdays, Av_weekday)\n",
    "plt.ylim([0, 35000])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(Hours, Av_hour)\n",
    "plt.ylim([0, 35000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a significant correlation between the day on the week and the energy consumed. Between the hour in the day and the energy consumed though, the variation seems too small to be taken in account.\n",
    "I think that it is a good idea to regress linearly the consumption on variables ‚ÄòHour‚Äô, but not on variable ‚ÄòWeekday‚Äô? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3) We will slightly transform the dataset. We first add a constant column for the regression (column of ones). You also convert the variable ‚ÄòHour‚Äô into 23 (and not 24!) binary variables H1,‚Ä¶H23. You next convert the variable ‚ÄòWeekday‚Äô into 6 (and not 7) variables W2,‚Ä¶,W7. Use the command get_dummies(.) from pandas. Why do we remove H0 and W1? Why do we do this conversion?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new features\n",
    "df[\"regression\"] = 1\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Hour'], prefix=\"H\", prefix_sep=\"\", drop_first=True)\n",
    "df = pd.get_dummies(df, columns=['Weekday'], prefix=\"W\", prefix_sep=\"\", drop_first=True)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we suspect a correlation between these variables and the load, make them binary variables allows us to easily switch from one to another in the regression model.\n",
    "With this idea, we can remove H0 and W1, because we'll accept them if all other options are rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "4) Split the dataset in\n",
    "\n",
    "‚Ä¢\ta training set with observations from 02-01-15 up to 01-12-2018 (included)\n",
    "\n",
    "‚Ä¢\ta test (or validation) set with observations from 02-12-2018 up to 31-12-2018.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset\n",
    "limit = datetime.datetime(2018, 12, 1)\n",
    "training_set = df[df[\"Date\"] <= limit]\n",
    "validation_set = df[df[\"Date\"] > limit]\n",
    "print(validation_set[\"Date\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "If we denote by L(t) the consumption at time t, the model that we want to develop is \n",
    "$$L(t) = cst + \\sum_{k=1}^{14} a_k L(t-k) +\\sum_{k=2}^{7} b_k W_k + \\sum_{k=1}^{23} c_k H_k    $$\n",
    "\n",
    "o\tEstimate this model with statsmodels on the training set. \n",
    "\n",
    "o\tHow would you judge the quality of the predictive model?\n",
    "\n",
    "o\tCompute the Mean Absolute Error (MAE) between predicted and real consumptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "last_14_hours_cols = [\"Lm\"+str(i) for i in range(1, 15)]\n",
    "weekdays_cols = [\"W\"+str(i)for i in range(2,8)]\n",
    "hours_in_day_cols = [\"H\"+str(i)for i in range(1,24)]\n",
    "\n",
    "X_training = training_set[[\"regression\"]+ last_14_hours_cols + weekdays_cols + hours_in_day_cols]\n",
    "Y_training = training_set[\"Load\"]\n",
    "\n",
    "model = sm.OLS(Y_training, X_training).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# Verifying efficacity on training set\n",
    "Y_predicted = model.predict(X_training)\n",
    "MAE_training = (Y_training-Y_predicted).abs().mean()\n",
    "mean_training = Y_training.mean()\n",
    "\n",
    "print(f\"\\nMAE on the training set = {round(MAE_training, 3)}\")\n",
    "print(f\"(Average Load = {round(mean_training, 3)})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Use this model on the test set to forecast the electric consumption.\n",
    "\n",
    "o\tCompare on a graph, the forecast to  real consumptions on the given period. Plot also the errors of prediction.\n",
    "\n",
    "o\tCompute the MAE on the test set and the R2. Is the forecast reliable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify efficacity on validation set\n",
    "X_bis = validation_set[[\"regression\"]+ last_14_hours_cols + weekdays_cols + hours_in_day_cols]\n",
    "Y_validation = validation_set[\"Load\"]\n",
    "Y_predicted = model.predict(X_bis)\n",
    "\n",
    "X = np.arange(len(validation_set))\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(X, Y_validation.to_list(), label=\"Real consumption\")\n",
    "plt.plot(X, Y_predicted.to_list(), label=\"Predicted consumption\")\n",
    "plt.plot(X, (Y_validation - Y_predicted).abs().to_list(), label=\"Absolute errors of prediction\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Hours\")\n",
    "plt.ylabel(\"Load\")\n",
    "plt.show()\n",
    "\n",
    "# Statistics analysis\n",
    "MAE_validation = (Y_validation-Y_predicted).abs().mean()\n",
    "mean_validation = Y_validation.mean()\n",
    "print(f\"\\nMAE on the validation set = {round(MAE_validation, 3)}\")\n",
    "print(f\"(Average Load = {round(mean_validation, 3)})\\n\")\n",
    "\n",
    "def get_real_R2(Y_real, Y_pred): # Recompute R2 from any sample\n",
    "    Y_mean = Y_real.mean()\n",
    "    Y_real = Y_real.to_list()\n",
    "    Y_pred = Y_pred.to_list()\n",
    "    SSR = 0.0\n",
    "    SST = 0.0\n",
    "    for i in range(len(Y_real)):\n",
    "        SSR += pow(Y_real[i] - Y_pred[i], 2)\n",
    "        SST += pow(Y_real[i] - Y_mean, 2)\n",
    "    if SST == 0 : return -1\n",
    "    return 1 - SSR/SST\n",
    "\n",
    "R2_validation = get_real_R2(Y_validation, Y_predicted)\n",
    "print(f\"R2 of our model is {model.rsquared_adj} on training set, and {R2_validation} on validation set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we recalculated manually the R2, because the one we get with our statsmodels.OLS summary is about the training set only.\n",
    "\n",
    "Since R2 is constantly above 97,5% for training and validation set, the forecast seems very reliable. In addition, we clearly see in the graphs that Predicted and real consumption are really close, and that the absolute error is really small compared to average values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We want to check that the average consumption at 11h is significantly different (at 95%) from the average consumption at 13h. Perform a statistical test (explain which stat you use) and report the stat value, the pivot value(s) and the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We so wanted to check if the assumption $H_0: \\mu_{11h} = \\mu_{13h};\\, H_1: \\mu_{11h} \\neq \\mu_{13h} $, and so we would do an analysis of variance by using a linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) The test of question 5 requires that variances are equal. Test if this assumption holds (95% confidence level). Report  the stat value, the pivot value (s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
